library(tidyverse)
library(scales)
tibble(heads = 0:10, prob = dbinom(x = 0:10, size = 10, prob = 0.5)) %>%
mutate(Heads = ifelse(heads == 8, "8", "other")) %>%
ggplot(aes(x = factor(heads), y = prob, fill = Heads)) +
theme_minimal()+
geom_col() +
geom_text(
aes(label = round(prob, 2), y = prob + 0.01),
position = position_dodge(0.9),
size = 3,
vjust = 0) +
labs(title = "Probability of X = 8 successes",
subtitle = "b(10, .5)",
x = "Successes (x)",
y = "probability")
tibble(rep = 1:10) %>%
#resampling the function as many times as you called in the line above
mutate(samples = map(rep, ~ rnorm(100))) %>%
#mean of each sample
mutate(means = map_dbl(samples, ~ mean(.)))
rerun(10, rnorm(100)) %>%
map_dbl(~ mean(.))
library(tidyverse)   # dplyr, tidyr, ggplot2, etc...
library(emmeans)
library(pROC)
library(lmerTest)
data('Mayflies', package='dsData')
install.packages("dsData")
devtools::install_github("DistanceDevelopment/Distance")
library(Distance)
data('Mayflies', package='Distance')
head(Mayflies)
Mayflies
list(5, 15, -5) %>%
map(rnorm, n = 5) %>%
str()
tribble(
~n, ~mean, ~sd,
10,    0,    1,
20,    5,    1,
30,    10,   2
) %>%
pmap(rnorm) %>%
str()
f <- c("runif", "rnorm", "rpois")
tribble(
~f,      ~params,
"runif", list(min = -1, max = 1,n=10),
"rnorm", list(sd = 5, n=15),
"rpois", list(lambda = 10,n=30)
) %>%
mutate(sim = invoke_map(f, params))
styler:::style_selection()
n_tosses <- 10
one_exp <- rbinom(p = 0.5, size = 1, n_tosses)
mean(one_exp)
sim <- tribble(
~f,      ~params,
"rbinom", list(size = 1, prob = 0.5)
)
sim %>%
mutate(sim = invoke_map(f, params, n = 10)) %>%
unnest(sim) %>%
summarise(mean_sim = mean(sim))
sim <- tribble(
~f,      ~params,
"rbinom", list(size = 1, prob = 0.5)
)
rep_sim <- sim %>%
crossing(rep = 1:1e5) %>%
mutate(sim = invoke_map(f, params, n = 10)) %>%
unnest(sim) %>% #flatten the list
group_by(rep) %>%
summarise(mean_sim = mean(sim))
head(rep_sim)
ggplot(rep_sim) +
aes(x = mean_sim) +
geom_histogram(bins = 10L, fill = "#6FBAD2") +
labs(
x = "Mean Heads",
y = "Count",
title = "Mean 'Heads' fom 10 Coin Tosses"
) +
theme_gray()
ggplot(rep_sim) +
aes(x = mean_sim) +
geom_bar() +
scale_x_binned() +
labs(
x = "Mean Heads",
y = "Count",
title = "Mean 'Heads' fom 10 Coin Tosses"
) +
theme_gray()
set.seed(42)
params <- tribble(
~size, ~prob,
1,     0.5
)
p <- params %>%
crossing(n=1:1e4) %>%
pmap(rbinom) %>%
enframe(name="num_toss",value="observation") %>%
unnest(observation) %>%
group_by(num_toss) %>%
summarise(vale = mean(observation))
p
ggplot(p, aes(num_toss, vale)) +
geom_point(stat = "identity", fill = "purple") +
labs(x = "Number of Heads", y = "Probability of Heads in 10 flips (p)") +
theme_minimal()
p %>%
ggplot(aes(num_toss, vale)) +
geom_point(stat = "identity", fill = "purple", alpha = 0.75) +
labs(
x = "Number of Heads",
y = "Probability of Heads in 10 flips (p)") +
theme_minimal()
p %>%
ggplot(aes(x = num_toss, y = vale)) +
geom_point() +
labs(
x = "x",
y = "y",
title = "Title"
) +
geom_hline(aes(yintercept = 0.5, colour = "pink")) +
#  geom_smooth(method=NULL , color="red", fill="#69b3a2", se=TRUE) +
theme_minimal()
p %>%
ggplot(aes(x = num_toss, y = vale)) +
geom_point() +
labs(
x = "x",
y = "y",
title = "Title"
) +
geom_hline(aes(yintercept = 0.5, colour = "pink")) +
geom_smooth(method=NULL , color="red", fill="#69b3a2", se=TRUE) +
theme_minimal()
p %>%
ggplot(aes(x = num_toss, y = vale)) +
geom_point() +
labs(
x = "x",
y = "y",
title = "Title"
) +
geom_hline(aes(yintercept = 0.5, colour = "pink")) +
theme_minimal()
styler:::style_selection()
sim <- tribble(
~n_tosses, ~f, ~params,
10, "rbinom", list(size = 1, prob = 0.5, n = 15),
30, "rbinom", list(size = 1, prob = 0.5, n = 30),
100, "rbinom", list(size = 1, prob = 0.5, n = 100),
1000, "rbinom", list(size = 1, prob = 0.5, n = 1000),
10000, "rbinom", list(size = 1, prob = 0.5, n = 1e4)
)
sim_rep <- sim %>%
crossing(replication = 1:50) %>%
mutate(sims = invoke_map(f, params)) %>%
unnest(sims) %>%
group_by(replication, n_tosses) %>%
summarise(avg = mean(sims))
sim_rep %>%
ggplot(aes(x = factor(n_tosses), y = avg)) +
ggbeeswarm::geom_quasirandom(color = "lightgrey") +
scale_y_continuous(limits = c(0, 1)) +
geom_hline(
yintercept = 0.5,
color = "skyblue", lty = 1, size = 1, alpha = 3 / 4
) +
ggthemes::theme_pander() +
labs(
title = "50 Replicates Of Mean 'Heads' As Number Of Tosses Increase",
y = "mean",
x = "Number Of Tosses"
)
styler:::style_selection()
library(Lahman)
set.seed(42)
set.seed(42)
# Select 30 players that don't have missing values
# and have a good amount of data (total hits > 500)
players <- Batting %>%
group_by(playerID) %>%
summarise(
AB_total = sum(AB),
H_total = sum(H)
) %>%
na.omit() %>%
filter(H_total > 500) %>% # collect players with a lot of data
sample_n(size = 30) %>% # randomly sample 30 players
pull(playerID)
TRUTH <- Batting %>%
filter(playerID %in% players) %>%
group_by(playerID) %>%
summarise(
AB_total = sum(AB),
H_total = sum(H)
) %>%
mutate(TRUTH = H_total / AB_total) %>%
select(playerID, TRUTH)
set.seed(42)
obs <- Batting %>%
filter(playerID %in% players) %>%
group_by(playerID) %>%
do(sample_n(., 5)) %>%
group_by(playerID) %>%
summarise(
AB_total = sum(AB),
H_total = sum(H)
) %>%
mutate(MLE = H_total / AB_total) %>%
select(playerID, MLE, AB_total) %>%
inner_join(TRUTH, by = "playerID")
p_ <- mean(obs$MLE)
N <- length(obs$MLE)
obs %>% summarise(median(AB_total))
df <- obs %>%
mutate(
sigma2 = (p_ * (1 - p_)) / 1624.5,
JS = p_ + (1 - ((N - 3) * sigma2 / (sum((MLE - p_)^2)))) * (MLE - p_)
) %>%
select(-AB_total, -sigma2)
df
errors <- df %>%
mutate(
mle_pred_error_i = (MLE - TRUTH)^2,
js_pred_error_i = (JS - TRUTH)^2
) %>%
summarise(
js_pred_error = sum(js_pred_error_i),
mle_pred_error = sum(mle_pred_error_i)
)
df
set.seed(42)
sim <- tibble(player = letters,
AB_total=300,
TRUTH = rbeta(100,300,n=26),
hits_list = map(.f=rbinom,TRUTH,n=300,size=1)
)
bats_sim <- sim %>%
unnest(hits_list) %>%
group_by(player) %>%
summarise(Hits = sum(hits_list)) %>%
inner_join(sim, by = "player") %>%
mutate(MLE = Hits/AB_total) %>%
select(-hits_list)
bats_sim
#calculate variables for JS estimator
p_=mean(bats_sim$MLE)
N = length(bats_sim$MLE)
bats_sim <- bats_sim %>%
mutate(sigma2 = (p_*(1-p_))/300,
JS=p_+(1-((N-3)*sigma2/(sum((MLE-p_)^2))))*(MLE-p_)) %>%
select(-AB_total,-Hits,-sigma2)
# plot
bats_sim %>%
gather(type,value,2:4) %>%
mutate(is_truth=+(type=="TRUTH")) %>%
mutate(type = factor(type, levels = c("TRUTH","JS","MLE"))) %>%
arrange(player, type) %>%
ggplot(aes(x=value,y=type))+
geom_path(aes(group=player),lty=2,color="lightgrey")+
scale_color_manual(values=c("lightgrey", "skyblue"))+ #truth==skyblue
geom_point(aes(color=factor(is_truth)))+
guides(color=FALSE)+ #remove legend
labs(title="The James-Stein Estimator Shrinks the MLE")+
theme_minimal()
errors <- bats_sim %>%
mutate(mle_pred_error_i = (MLE-TRUTH)^2,
js_pred_error_i = (JS-TRUTH)^2) %>%
summarise(js_pred_error = sum(js_pred_error_i),
mle_pred_error = sum(mle_pred_error_i))
errors
library(h2o)
library(tidyverse)
library(janitor)
library(readr)
white_wine <- read_csv("white_wine.csv")
red_wine <- read_csv("red_wine.csv")
wine_df <- bind_rows(red_wine, white_wine) %>%
clean_names() %>%
mutate(
wine_class = ifelse(quality >= 7, "IdDrinkThat", "NoThanks")
) %>%
mutate(wine_class = as.factor(wine_class)) %>%
select(-quality)
trainTestSplit <- function(df, split_proportion) {
set.seed(123)
out_list <- list()
data_split <- sample(
1:2,
size = nrow(df),
prob = split_proportion,
replace = TRUE
)
out_list$train <- df[data_split == 1, ]
out_list$test <- df[data_split == 2, ]
return(out_list)
}
split_proportion <- c(0.7, 0.3)
df_split <- trainTestSplit(wine_df, split_proportion)
h2o.init(nthreads = -1)
train_h2o <- as.h2o(df_split$train)
test_h2o <- as.h2o(df_split$test)
y_var <- "wine_class"
x_var <- setdiff(names(train_h2o), y_var)
gbm_fit = h2o.gbm(
x = x_var,
y = y_var,
distribution = "bernoulli",
training_frame = train_h2o,
stopping_rounds = 3,
stopping_metric = "AUC",
verbose = FALSE
)
data.frame(gbm_fit@model$variable_importances) %>%
select(variable, relative_importance) %>%
mutate(
relative_importance = round(relative_importance),
variable = factor(variable)
) %>%
mutate(
variable = fct_reorder(variable, relative_importance, .desc = TRUE)
) %>%
ggplot(aes(
x = variable,
y = relative_importance,
fill = variable,
label = as.character(relative_importance)
)) +
geom_bar(stat = "identity") +
geom_label(label.size = 1, size = 5, color = "white") +
theme_bw() +
theme(legend.position = "none") +
ylab("Relative Importance") +
xlab("Variable") +
theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 12))
gbm_auc <- h2o.auc(h2o.performance(gbm_fit, newdata = test_h2o))
rf_fit <- h2o.randomForest(
x = x_var,
y = y_var,
training_frame = train_h2o,
stopping_rounds = 3,
stopping_metric = "AUC"
)
rf_auc <- h2o.auc(h2o.performance(rf_fit, newdata = test_h2o))
nn_fit <- h2o.deeplearning(
x = x_var,
y = y_var,
training_frame = train_h2o,
stopping_rounds = 3,
stopping_metric = "AUC"
)
nn_auc <- h2o.auc(h2o.performance(nn_fit, newdata = test_h2o))
glm_fit <- h2o.glm(
x = x_var,
y = y_var,
family = "binomial",
training_frame = train_h2o
)
glm_auc <- h2o.auc(h2o.performance(glm_fit, newdata = test_h2o))
auc_df <- data.frame(
n_noise_vars = rep(0, 4),
method = c("gbm", "rf", "nn", "glm"),
AUC = c(gbm_auc, rf_auc, nn_auc, glm_auc)
) %>%
arrange(desc(AUC))
print(round(table(wine_df$wine_class)/nrow(wine_df) * 100, 1))
n_noise_vars <- seq(100, 2000, length.out = 20)
for (i in n_noise_vars) {
print(i)
temp_noise_df_train <- data.frame(placeholder = rep(NA, nrow(df_split$train)))
temp_noise_df_test <- data.frame(placeholder = rep(NA, nrow(df_split$test)))
# add in i irrelevant predictors to train and test
for (j in 1:i) {
temp_noise_df_train <- cbind(
temp_noise_df_train,
data.frame(noise.var = rnorm(
nrow(df_split$train),
0,
1
))
)
temp_noise_df_test <- cbind(
temp_noise_df_test,
data.frame(noise.var = rnorm(
nrow(df_split$test),
0,
1
))
)
}
# format names of irrelevant variables
temp_noise_df_train <- temp_noise_df_train[, 2:dim(temp_noise_df_train)[2]]
names(temp_noise_df_train) <- gsub("\\.", "", names(temp_noise_df_train))
temp_noise_df_train <- as.h2o(cbind(
temp_noise_df_train,
df_split$train
))
temp_noise_df_test <- temp_noise_df_test[, 2:dim(temp_noise_df_test)[2]]
names(temp_noise_df_test) <- gsub("\\.", "", names(temp_noise_df_test))
temp_noise_df_test <- cbind(
temp_noise_df_test,
df_split$test
)
x_var <- setdiff(names(temp_noise_df_train), y_var)
gbm_fit <- h2o.gbm(
x = x_var,
y = y_var,
distribution = "bernoulli",
training_frame = temp_noise_df_train,
stopping_rounds = 3,
stopping_metric = "AUC"
)
rf_fit <- h2o.randomForest(
x = x_var,
y = y_var,
training_frame = temp_noise_df_train,
stopping_rounds = 3,
stopping_metric = "AUC"
)
nn_fit <- h2o.deeplearning(
x = x_var,
y = y_var,
training_frame = temp_noise_df_train,
stopping_rounds = 3,
stopping_metric = "AUC"
)
glm_fit <- h2o.glm(
x = x_var,
y = y_var,
family = "binomial",
training_frame = temp_noise_df_train
)
temp_noise_df_test <- as.h2o(temp_noise_df_test)
gbm_auc <- h2o.auc(h2o.performance(gbm_fit, newdata = temp_noise_df_test))
rf_auc <- h2o.auc(h2o.performance(rf_fit, newdata = temp_noise_df_test))
nn_auc <- h2o.auc(h2o.performance(nn_fit, newdata = temp_noise_df_test))
glm_auc <- h2o.auc(h2o.performance(glm_fit, newdata = temp_noise_df_test))
auc_df <- rbind(
auc_df,
data.frame(
n_noise_vars = i,
method = c("gbm", "rf", "nn", "glm"),
AUC = c(gbm_auc, rf_auc, nn_auc, glm_auc)
)
)
}
auc_df %>%
ggplot(aes(
x = n_noise_vars,
y = AUC,
color = method
)) +
geom_point(size = 2) +
stat_smooth(span = 1.75, se = FALSE,  size = 2) +
theme_bw() +
labs(
x = 'N Noise Variables',
y = 'AUC'
)

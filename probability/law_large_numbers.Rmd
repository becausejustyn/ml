---
title: "R Notebook"
output: html_notebook
---

https://bookdown.org/content/922/tidyverse.html#law-of-large-numbers-tidyverse-practice

## Law of Large Numbers

We will want to make draws from the same distribution, but with different parameters. Lets generate three random normals with means 5, 15, -5.

```{r}
mu <- list(5, 15, -5)
mu %>% 
  map(rnorm, n = 5) %>% 
  str()
```

We may want to specify two parameters in a distribution.
```{r}
params <- tribble(
 ~n, ~mean, ~sd, 
 10,    0,    1,     
 20,    5,    1,     
 30,    10,   2      
)
params %>% 
  pmap(rnorm) %>% 
  str()
```

The pmap() example was still simplistic. We could have added complexity by adding more than one distribution, each with different parameterizations.

```{r}
f <- c("runif", "rnorm", "rpois")

sim <- tribble(
  ~f,      ~params,                  
  "runif", list(min = -1, max = 1,n=10),  
  "rnorm", list(sd = 5, n=15),             
  "rpois", list(lambda = 10,n=30)   
)
sim %>% 
  mutate(sim = invoke_map(f, params))
```

Coin Toss Example
```{r}
#1. One Experiment
n_tosses=10
one_exp <- rbinom(p=0.5, size=1, n_tosses)
mean(one_exp)
```
The following code does the same thing as the code above it, but we set it up slightly different to it will be easy for us to add replications.

```{r}
#2. Setup For Replication
sim <- tribble(
  ~f,      ~params,
  "rbinom", list(size = 1, prob = 0.5)
)

sim %>% 
  mutate(sim = invoke_map(f, params, n = 10)) %>% 
  unnest(sim) %>% 
  summarise(mean_sim = mean(sim))
```

Now lets add some replications.

```{r}
#3. Replicate 1e4 Times 
sim <- tribble(
  ~f,      ~params,
  "rbinom", list(size = 1, prob = 0.5)
)
rep_sim <- sim %>% 
  crossing(rep = 1:1e5) %>%
  mutate(sim = invoke_map(f, params, n = 10)) %>% 
  unnest(sim) %>% #flatten the list
  group_by(rep) %>% 
  summarise(mean_sim = mean(sim))
head(rep_sim)
```

```{r}
ggplot(rep_sim) +
  aes(x = mean_sim) +
  geom_histogram(bins = 10L, fill = "#6FBAD2") +
  labs(
    x = "Mean Heads",
    y = "Count",
    title = "Mean 'Heads' fom 10 Coin Tosses"
  ) +
  theme_gray()
```

```{r}
ggplot(rep_sim) +
  aes(x = mean_sim) +
  geom_bar() +
  scale_x_binned() +
  labs(
    x = "Mean Heads",
    y = "Count",
    title = "Mean 'Heads' fom 10 Coin Tosses"
  ) +
  theme_gray()
```

```{r}
set.seed(42)
params <- tribble(
  ~size, ~prob,
    1,     0.5
)
p <- params %>% 
  crossing(n=1:1e4) %>% 
  pmap(rbinom) %>% 
  enframe(name="num_toss",value="observation") 
```

```{r}
p <- params %>% 
  crossing(n=1:1e4) %>% 
  pmap(rbinom) %>% 
  enframe(name="num_toss",value="observation") %>%  
  unnest(observation) %>% 
  group_by(num_toss) %>% 
  summarise(vale = mean(observation))
```

```{r}
ggplot(p, aes(num_toss, vale)) + 
  geom_point(stat = "identity", fill = "purple") + 
  labs(x = "Number of Heads", y = "Probability of Heads in 10 flips (p)") +
  theme_minimal()
```

```{r}
ggplot(p) +
  aes(x = num_toss, y = vale) +
  geom_point() +
  labs(
    x = "x",
    y = "y",
    title = "Title"
  ) +
  geom_hline(aes(yintercept = 0.5, colour = "pink")) +
#  geom_smooth(method=NULL , color="red", fill="#69b3a2", se=TRUE) +
  theme_minimal()
```

```{r}
sim <- tribble(
 ~n_tosses, ~f,       ~params,
  10, "rbinom", list(size=1,prob=0.5,n=15), 
  30, "rbinom", list(size=1,prob=0.5,n=30), 
  100, "rbinom", list(size=1,prob=0.5,n=100), 
  1000, "rbinom", list(size=1,prob=0.5,n=1000),
  10000, "rbinom", list(size=1,prob=0.5,n=1e4)
)
sim_rep <- sim %>%
  crossing(replication=1:50) %>% 
  mutate(sims = invoke_map(f, params)) %>% 
  unnest(sims) %>% 
  group_by(replication,n_tosses) %>% 
  summarise(avg = mean(sims)) 
```

```{r}
sim_rep %>% 
ggplot(aes(x=factor(n_tosses),y=avg))+
  ggbeeswarm::geom_quasirandom(color="lightgrey")+
  scale_y_continuous(limits = c(0,1))+
  geom_hline(yintercept = 0.5, 
             color = "skyblue",lty=1,size=1,alpha=3/4)+
  ggthemes::theme_pander()+
  labs(title="50 Replicates Of Mean 'Heads' As Number Of Tosses Increase",
       y="mean",
       x="Number Of Tosses")
```

## James-Stein Theorem

### Data
```{r}
library(Lahman)
set.seed(42)

# 1.
# Select 30 players that don't have missing values
# and have a good amount of data (total hits > 500)
players <- Batting %>% 
  group_by(playerID) %>% 
  summarise(AB_total = sum(AB), 
            H_total = sum(H)) %>% 
  na.omit() %>% 
  filter(H_total>500) %>%  # collect players with a lot of data
  sample_n(size=30) %>%    # randomly sample 30 players
  pull(playerID)           # `select` will produce a dataframe, 
                           # `pull` gives a list which will be easier
                           # to work with when we `filter` by players
```

```{r}
# 2. Get "truth"
TRUTH <- Batting %>% 
  filter(playerID %in% players) %>% 
  group_by(playerID) %>% 
  summarise(AB_total = sum(AB), 
            H_total = sum(H)) %>% 
  mutate(TRUTH = H_total/AB_total) %>% 
  select(playerID,TRUTH)
```

```{r}
# 3. Collect 5 observations per player 
# 4.Predict truth with MLE and James-Stein Estimator
set.seed(42)
obs <- Batting %>% 
  filter(playerID %in% players) %>% 
  group_by(playerID) %>%
  do(sample_n(., 5))  %>%       
  group_by(playerID) %>% 
  summarise(AB_total = sum(AB), 
            H_total = sum(H)) %>% 
  mutate(MLE = H_total/AB_total) %>% 
  select(playerID,MLE,AB_total) %>% 
  inner_join(TRUTH,by="playerID")
```

```{r}
# Define variables for James-Stein estimator
p_=mean(obs$MLE)
N = length(obs$MLE)
obs %>% summarise(median(AB_total))
```

```{r}
df <- obs %>% 
  mutate(sigma2 = (p_*(1-p_))/1624.5,
         JS=p_+(1-((N-3)*sigma2/(sum((MLE-p_)^2))))*(MLE-p_)) %>% 
  select(-AB_total,-sigma2)

head(df)
```

```{r}
errors <- df %>% 
  mutate(mle_pred_error_i = (MLE-TRUTH)^2,
         js_pred_error_i = (JS-TRUTH)^2) %>% 
  summarise(js_pred_error = sum(js_pred_error_i),
            mle_pred_error = sum(mle_pred_error_i))
```

#### Binomial Approximation

```{r}
# build data with MLE estiamte
set.seed(42)
sim <- tibble(player = letters, 
              AB_total=300, 
              TRUTH = rbeta(100,300,n=26),
              hits_list = map(.f=rbinom,TRUTH,n=300,size=1)
)
bats_sim <- sim %>% 
  unnest(hits_list) %>% 
  group_by(player) %>% 
  summarise(Hits = sum(hits_list)) %>% 
  inner_join(sim, by = "player") %>% 
  mutate(MLE = Hits/AB_total) %>% 
  select(-hits_list)
bats_sim
```

```{r}
#calculate variables for JS estimator
p_=mean(bats_sim$MLE)
N = length(bats_sim$MLE)

# add JS to simulated baseball data 
bats_sim <- bats_sim %>% 
  mutate(sigma2 = (p_*(1-p_))/300,
         JS=p_+(1-((N-3)*sigma2/(sum((MLE-p_)^2))))*(MLE-p_)) %>% 
  select(-AB_total,-Hits,-sigma2)
```

```{r}
# plot
bats_sim %>% 
  gather(type,value,2:4) %>% 
  mutate(is_truth=+(type=="TRUTH")) %>% 
  mutate(type = factor(type, levels = c("TRUTH","JS","MLE"))) %>%
  arrange(player, type) %>% 
  ggplot(aes(x=value,y=type))+
  geom_path(aes(group=player),lty=2,color="lightgrey")+
  scale_color_manual(values=c("lightgrey", "skyblue"))+ #truth==skyblue
  geom_point(aes(color=factor(is_truth)))+
  guides(color=FALSE)+ #remove legend
  labs(title="The James-Stein Estimator Shrinks the MLE")+
  theme_minimal()
```

```{r}
errors <- bats_sim %>% 
  mutate(mle_pred_error_i = (MLE-TRUTH)^2,
         js_pred_error_i = (JS-TRUTH)^2) %>% 
  summarise(js_pred_error = sum(js_pred_error_i),
            mle_pred_error = sum(mle_pred_error_i))
errors
```

### Figure this out
```{r}
# First, create 1000 datasets of N=100 each from a normal distribution.
set.seed(5)
sims <- 1000
datasets <- list()
for(i in 1:sims) {
  datasets[[i]] <- rnorm(100)
}
```

```{r}
  # Now get the 95% confidence interval for each dataset.
out <- matrix(unlist(lapply(datasets, function(x) t.test(x)$conf.int)), ncol=2, byrow=T)
colnames(out) <- c("LL", "UL")
  # Count the number of confidence intervals containing Mu
res <- ifelse(out[,1] <= 0 & out[,2] >= 0, 1, 0)
sum(res) / sims
  # Thus, ~95% of our CIs contain Mu
```

```{r}
# Now get the sample Means
Ms <- unlist(lapply(datasets, mean))
    # For each confidence interval, determine how many other sample means it captured
reptest <- sapply(Ms, function(x) ifelse(out[,1] <= x & out[,2] >= x, 1, 0))
      # Remove the diagonal to avoid double-counting
diag(reptest) <- NA
      # Now summarize it:
mean(colMeans(reptest, na.rm=T)) # So ~ 84.4% chance of a replication falling within the 95% CI
```

